{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [HSLS] Baseline -- Disparate Mistreatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "sys.path.append('../../../fair-classification_python3/fair_classification') # the code for fair classification is in this directory\n",
    "sys.path.append(\"../\")\n",
    "cwd = '../../../core'\n",
    "sys.path.append(cwd)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils as ut\n",
    "import funcs_disp_mist as fdm\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from missing_module import * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms = pd.read_pickle('pkl_data/hsls_orig.pkl')\n",
    "\n",
    "sens_attr = 'racebin'\n",
    "privileged_groups = [{'racebin': 1}]\n",
    "unprivileged_groups = [{'racebin': 0}]\n",
    "\n",
    "df_ms['gradebin'].replace({0: -1}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Imputation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fair = 'fnr'\n",
    "tau_list = [500]\n",
    "# tau_list = [0.1]\n",
    "\n",
    "fr_mean, acc_mean, fr_std, acc_std = [], [], [], [] \n",
    "\n",
    "\n",
    "for tau in tau_list: \n",
    "    fr_list = []\n",
    "    acc_list = [] \n",
    "    \n",
    "    for seed in range (1, 11): \n",
    "        print(tau,seed)\n",
    "        ################## Train-Test Split ################### \n",
    "        dataset_orig_train, dataset_orig_test = train_test_split(df_ms, test_size=0.3, random_state=seed)\n",
    "\n",
    "        ########################################################\n",
    "\n",
    "        ##################### Imputation ###################### \n",
    "        imputer = SimpleImputer()\n",
    "\n",
    "        dataset_orig_train_no_sens = dataset_orig_train.drop(columns=['racebin','gradebin'])\n",
    "        dataset_orig_test_no_sens = dataset_orig_test.drop(columns=['racebin','gradebin'])\n",
    "\n",
    "\n",
    "        dataset_orig_train_no_sens = pd.DataFrame(imputer.fit_transform(dataset_orig_train_no_sens), \n",
    "                                                  columns=dataset_orig_train_no_sens.columns, \n",
    "                                                  index=dataset_orig_train_no_sens.index)\n",
    "        dataset_orig_test_no_sens = pd.DataFrame(imputer.transform(dataset_orig_test_no_sens), \n",
    "                                                 columns=dataset_orig_test_no_sens.columns, \n",
    "                                                 index=dataset_orig_test_no_sens.index)\n",
    "        dataset_orig_train = pd.concat([dataset_orig_train_no_sens, dataset_orig_train[['racebin','gradebin']]], axis=1)\n",
    "        dataset_orig_test = pd.concat([dataset_orig_test_no_sens, dataset_orig_test[['racebin','gradebin']]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        ########################################################\n",
    "        sensitive_attrs = ['racebin']\n",
    "\n",
    "        X_train = dataset_orig_train.iloc[:,:-1]\n",
    "        X_train['intercept'] = np.ones(len(X_train))\n",
    "        x_control_train = dict({'racebin': np.array([int(x) for x in X_train['racebin']])})\n",
    "        X_train = np.array(X_train.drop(columns=['racebin']))\n",
    "        y_train = np.array(dataset_orig_train.iloc[:,-1])\n",
    "\n",
    "        X_test = dataset_orig_test.iloc[:,:-1]\n",
    "        X_test['intercept'] = np.ones(len(X_test))\n",
    "        x_control_test = dict({'racebin': np.array([int(x) for x in X_test['racebin']])})\n",
    "        X_test = np.array(X_test.drop(columns=['racebin']))\n",
    "        y_test = np.array(dataset_orig_test.iloc[:,-1])\n",
    "\n",
    "\n",
    "        ##########################################################\n",
    "\n",
    "        ################ Disparate Mistreatment ################## \n",
    "        if fair == 'fpr':\n",
    "            cons_type = 1 \n",
    "        elif fair == 'fnr': \n",
    "            cons_type = 2\n",
    "        elif fair == 'acc':\n",
    "            cons_type = 0 \n",
    "        elif fair == 'eqodds':\n",
    "            cons_type = 4 \n",
    "\n",
    "        mu = 1.2\n",
    "        loss_function = \"logreg\" # perform the experiments with logistic regression\n",
    "        EPS = 1e-6\n",
    "\n",
    "        sensitive_attrs_to_cov_thresh = {\"racebin\": {0:{0:0, 1:0}, 1:{0:0, 1:0}, 2:{0:0, 1:0}} } # zero covariance threshold, means try to get the fairest solution\n",
    "        cons_params = {\"cons_type\": cons_type, \n",
    "                       \"tau\": tau, \n",
    "                       \"mu\": mu, \n",
    "                       \"sensitive_attrs_to_cov_thresh\": sensitive_attrs_to_cov_thresh}\n",
    "        try:\n",
    "            w = fdm.train_model_disp_mist(X_train, y_train, x_control_train, loss_function, EPS, cons_params)\n",
    "        except:\n",
    "            pass\n",
    "        train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test = fdm.get_clf_stats(w, X_train, y_train, x_control_train, X_test, y_test, x_control_test, sensitive_attrs)\n",
    "\n",
    "        fr = np.abs(s_attr_to_fp_fn_test['racebin'][0][fair] - s_attr_to_fp_fn_test['racebin'][1][fair])\n",
    "        fr_list.append(fr)\n",
    "        acc_list.append(test_score)\n",
    "        \n",
    "    fr_mean.append(np.mean(fr_list))\n",
    "    fr_std.append(np.std(fr_list))\n",
    "    acc_mean.append(np.mean(acc_list))\n",
    "    acc_std.append(np.std(acc_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('disp_mistrtment_result.pkl', 'wb+') as f: \n",
    "    pickle.dump({'fr_mean': fr_mean, 'fr_std': fr_std, 'acc_mean': acc_mean, 'acc_std': acc_std}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <br/>\n",
    "\n",
    "## KNN Imputation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fair = 'fnr'\n",
    "tau_list = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "fr_mean, acc_mean, fr_std, acc_std = [], [], [], [] \n",
    "\n",
    "\n",
    "for tau in tau_list: \n",
    "    fr_list = []\n",
    "    acc_list = [] \n",
    "    \n",
    "    for seed in range (1, 11): \n",
    "        print(tau,seed)\n",
    "        ################## Train-Test Split ################### \n",
    "        dataset_orig_train, dataset_orig_test = train_test_split(df_ms, test_size=0.3, random_state=seed)\n",
    "\n",
    "        ########################################################\n",
    "\n",
    "        ##################### Imputation ###################### \n",
    "        imputer = KNNImputer()\n",
    "\n",
    "        dataset_orig_train_no_sens = dataset_orig_train.drop(columns=['racebin','gradebin'])\n",
    "        dataset_orig_test_no_sens = dataset_orig_test.drop(columns=['racebin','gradebin'])\n",
    "\n",
    "\n",
    "        dataset_orig_train_no_sens = pd.DataFrame(imputer.fit_transform(dataset_orig_train_no_sens), \n",
    "                                                  columns=dataset_orig_train_no_sens.columns, \n",
    "                                                  index=dataset_orig_train_no_sens.index)\n",
    "        dataset_orig_test_no_sens = pd.DataFrame(imputer.transform(dataset_orig_test_no_sens), \n",
    "                                                 columns=dataset_orig_test_no_sens.columns, \n",
    "                                                 index=dataset_orig_test_no_sens.index)\n",
    "        dataset_orig_train = pd.concat([dataset_orig_train_no_sens, dataset_orig_train[['racebin','gradebin']]], axis=1)\n",
    "        dataset_orig_test = pd.concat([dataset_orig_test_no_sens, dataset_orig_test[['racebin','gradebin']]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        ########################################################\n",
    "        sensitive_attrs = ['racebin']\n",
    "\n",
    "        X_train = dataset_orig_train.iloc[:,:-1]\n",
    "        X_train['intercept'] = np.ones(len(X_train))\n",
    "        x_control_train = dict({'racebin': np.array([int(x) for x in X_train['racebin']])})\n",
    "        X_train = np.array(X_train.drop(columns=['racebin']))\n",
    "        y_train = np.array(dataset_orig_train.iloc[:,-1])\n",
    "\n",
    "        print(X_train.shape)\n",
    "\n",
    "        X_test = dataset_orig_test.iloc[:,:-1]\n",
    "        X_test['intercept'] = np.ones(len(X_test))\n",
    "        x_control_test = dict({'racebin': np.array([int(x) for x in X_test['racebin']])})\n",
    "        X_test = np.array(X_test.drop(columns=['racebin']))\n",
    "        y_test = np.array(dataset_orig_test.iloc[:,-1])\n",
    "\n",
    "\n",
    "        ##########################################################\n",
    "\n",
    "        ################ Disparate Mistreatment ################## \n",
    "        if fair == 'fpr':\n",
    "            cons_type = 1 \n",
    "        elif fair == 'fnr': \n",
    "            cons_type = 2\n",
    "        elif fair == 'acc':\n",
    "            cons_type = 0 \n",
    "        elif fair == 'eqodds':\n",
    "            cons_type = 4 \n",
    "\n",
    "        mu = 1.2\n",
    "        loss_function = \"logreg\" # perform the experiments with logistic regression\n",
    "        EPS = 1e-6\n",
    "\n",
    "        sensitive_attrs_to_cov_thresh = {\"racebin\": {0:{0:0, 1:0}, 1:{0:0, 1:0}, 2:{0:0, 1:0}} } # zero covariance threshold, means try to get the fairest solution\n",
    "        cons_params = {\"cons_type\": cons_type, \n",
    "                       \"tau\": tau, \n",
    "                       \"mu\": mu, \n",
    "                       \"sensitive_attrs_to_cov_thresh\": sensitive_attrs_to_cov_thresh}\n",
    "        try:\n",
    "            w = fdm.train_model_disp_mist(X_train, y_train, x_control_train, loss_function, EPS, cons_params)\n",
    "        except:\n",
    "            pass\n",
    "        train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test = fdm.get_clf_stats(w, X_train, y_train, x_control_train, X_test, y_test, x_control_test, sensitive_attrs)\n",
    "\n",
    "        fr = np.abs(s_attr_to_fp_fn_test['racebin'][0][fair] - s_attr_to_fp_fn_test['racebin'][1][fair])\n",
    "        fr_list.append(fr)\n",
    "        acc_list.append(test_score)\n",
    "        \n",
    "    fr_mean.append(np.mean(fr_list))\n",
    "    fr_std.append(np.std(fr_list))\n",
    "    acc_mean.append(np.mean(acc_list))\n",
    "    acc_std.append(np.std(acc_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('knn_disp_mistrtment_result.pkl', 'wb+') as f: \n",
    "    pickle.dump({'fr_mean': fr_mean, 'fr_std': fr_std, 'acc_mean': acc_mean, 'acc_std': acc_std}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
