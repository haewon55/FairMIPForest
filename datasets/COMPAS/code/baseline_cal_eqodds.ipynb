{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COMPAS] Baseline -- Equalized Odds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "cwd = '../../../core'\n",
    "sys.path.append(cwd)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset, StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "                import load_preproc_data_adult, load_preproc_data_compas\n",
    "\n",
    "\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing.eq_odds_postprocessing import EqOddsPostprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from load_compas import * \n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from load_compas import * \n",
    "from missing_module import *\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of people recidivating within two years\n",
      "-1    2795\n",
      " 1    2483\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "('Features we will be using for classification are:', ['age_cat_25 - 45', 'age_cat_Greater than 45', 'age_cat_Less than 25', 'race', 'sex', 'priors_count', 'c_charge_degree'], '\\n')\n",
      "Int64Index([0, 1, 2, 8, 10, 13, 14, 15, 21, 22], dtype='int64')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_cat_25 - 45</th>\n",
       "      <th>age_cat_Greater than 45</th>\n",
       "      <th>age_cat_Less than 25</th>\n",
       "      <th>sex</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.592011</td>\n",
       "      <td>0.147408</td>\n",
       "      <td>0.260580</td>\n",
       "      <td>0.820127</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>0.308131</td>\n",
       "      <td>0.533999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.536377</td>\n",
       "      <td>0.298621</td>\n",
       "      <td>0.165002</td>\n",
       "      <td>0.771791</td>\n",
       "      <td>-0.245055</td>\n",
       "      <td>0.408464</td>\n",
       "      <td>0.390870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age_cat_25 - 45  age_cat_Greater than 45  age_cat_Less than 25  \\\n",
       "race                                                                   \n",
       "0.0          0.592011                 0.147408              0.260580   \n",
       "1.0          0.536377                 0.298621              0.165002   \n",
       "\n",
       "           sex  priors_count  c_charge_degree  two_year_recid  \n",
       "race                                                           \n",
       "0.0   0.820127      0.134560         0.308131        0.533999  \n",
       "1.0   0.771791     -0.245055         0.408464        0.390870  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, x_control = load_compas_data()\n",
    "\n",
    "df = pd.DataFrame(X, columns= ['age_cat_25 - 45', 'age_cat_Greater than 45', 'age_cat_Less than 25', 'race', 'sex', \n",
    "                               'priors_count', 'c_charge_degree'])\n",
    "\n",
    "idxx = df[df['race']==0].index\n",
    "print(idxx[:10])\n",
    "\n",
    "y = pd.Series(y, name=\"two_year_recid\")\n",
    "y[y==-1] = 0\n",
    "\n",
    "df = pd.concat([df, y], axis=1)\n",
    "df_bal = balance_data(df, 'race', 0)\n",
    "\n",
    "s = 777\n",
    "df_ms = generate_missing(df_bal, c_label='race', ms_label='sex', p_ms0=0.4, p_ms1=0.1, seed=s)\n",
    "df_ms = generate_missing(df_ms, c_label='race', ms_label='priors_count', p_ms0=0.6, p_ms1=0.2, seed=s)\n",
    "\n",
    "\n",
    "privileged_groups = [{'race': 1}]\n",
    "unprivileged_groups = [{'race': 0}]\n",
    "\n",
    "df_ms.groupby(df_ms['race']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_constraint = \"fnr\"\n",
    "\n",
    "privileged_groups = [{'race': 1}]\n",
    "unprivileged_groups = [{'race': 0}]\n",
    "\n",
    "favorable_label = 1\n",
    "randseed = 42 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original-Predicted testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "-0.19815097216927183\n",
      "Overall Test Accuracy \n",
      "0.5990491283676703\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "0.1219977125428898\n",
      "Overall Test Accuracy \n",
      "0.5689381933438986\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Predicted testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "-0.09499999999999997\n",
      "Overall Test Accuracy \n",
      "0.6085578446909667\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "0.031000000000000028\n",
      "Overall Test Accuracy \n",
      "0.5752773375594294\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Predicted testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "-0.2241531938685179\n",
      "Overall Test Accuracy \n",
      "0.6275752773375595\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "0.03513022410660205\n",
      "Overall Test Accuracy \n",
      "0.5832012678288431\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Predicted testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "-0.23200591333271736\n",
      "Overall Test Accuracy \n",
      "0.6101426307448494\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "0.08491176198835815\n",
      "Overall Test Accuracy \n",
      "0.595879556259905\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Predicted testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "-0.20166119374154912\n",
      "Overall Test Accuracy \n",
      "0.606973058637084\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "0.034624299787521684\n",
      "Overall Test Accuracy \n",
      "0.606973058637084\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Predicted testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "-0.2728116300649551\n",
      "Overall Test Accuracy \n",
      "0.6228209191759112\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "0.04892256933704503\n",
      "Overall Test Accuracy \n",
      "0.5768621236133122\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Predicted testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "-0.16527965037891112\n",
      "Overall Test Accuracy \n",
      "0.5927099841521395\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "0.009484401878283477\n",
      "Overall Test Accuracy \n",
      "0.572107765451664\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Predicted testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "-0.26348258706467664\n",
      "Overall Test Accuracy \n",
      "0.5657686212361331\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "0.016417910447761197\n",
      "Overall Test Accuracy \n",
      "0.5404120443740095\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Predicted testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "-0.29367746560917685\n",
      "Overall Test Accuracy \n",
      "0.6307448494453248\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "0.09987901599677376\n",
      "Overall Test Accuracy \n",
      "0.5625990491283677\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Predicted testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "-0.32853167457849075\n",
      "Overall Test Accuracy \n",
      "0.5832012678288431\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in FPR between unprivileged and privileged groups\n",
      "0.017515141594368955\n",
      "Overall Test Accuracy \n",
      "0.5372424722662441\n"
     ]
    }
   ],
   "source": [
    "fr_list = []\n",
    "acc_list = [] \n",
    "for seed in range (1, 11): \n",
    "    dataset_orig_train, dataset_orig_test = train_test_split(df_ms, test_size=0.3, random_state=seed)\n",
    "\n",
    "    dataset_orig_train_no_sens = dataset_orig_train.drop(columns=['race','two_year_recid'])\n",
    "    dataset_orig_test_no_sens = dataset_orig_test.drop(columns=['race','two_year_recid'])\n",
    "\n",
    "    \n",
    "    ## Change the following two lines to get mean or k-nn results ##\n",
    "    imputer = SimpleImputer()\n",
    "#     imputer = KNNImputer()\n",
    "\n",
    "    dataset_orig_train_no_sens = pd.DataFrame(imputer.fit_transform(dataset_orig_train_no_sens), \n",
    "                                              columns=dataset_orig_train_no_sens.columns, \n",
    "                                              index=dataset_orig_train_no_sens.index)\n",
    "    dataset_orig_test_no_sens = pd.DataFrame(imputer.transform(dataset_orig_test_no_sens), \n",
    "                                             columns=dataset_orig_test_no_sens.columns, \n",
    "                                             index=dataset_orig_test_no_sens.index)\n",
    "    dataset_orig_train = pd.concat([dataset_orig_train_no_sens, dataset_orig_train[['race','two_year_recid']]], axis=1)\n",
    "    dataset_orig_test = pd.concat([dataset_orig_test_no_sens, dataset_orig_test[['race','two_year_recid']]], axis=1)\n",
    "\n",
    "\n",
    "    dataset_orig_valid, dataset_orig_test = train_test_split(dataset_orig_test, test_size=0.5, random_state=seed)\n",
    "    \n",
    "\n",
    "    ### Converting to AIF360 StandardDataset objects ###\n",
    "    dataset_orig_train = StandardDataset(dataset_orig_train, label_name='two_year_recid', favorable_classes=[1],\n",
    "                                         protected_attribute_names=['race'], privileged_classes=[[1]])\n",
    "    dataset_orig_valid = StandardDataset(dataset_orig_valid, label_name='two_year_recid', favorable_classes=[1],\n",
    "                                         protected_attribute_names=['race'], privileged_classes=[[1]])\n",
    "    dataset_orig_test = StandardDataset(dataset_orig_test, label_name='two_year_recid', favorable_classes=[1],\n",
    "                                         protected_attribute_names=['race'], privileged_classes=[[1]])\n",
    "\n",
    "    # Placeholder for predicted and transformed datasets\n",
    "    dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "    dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "    dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "\n",
    "    dataset_new_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "    dataset_new_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "\n",
    "    X_train = dataset_orig_train.features[:,:-1]\n",
    "    y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "    X_train.shape\n",
    "    # lmod = LogisticRegression()\n",
    "    lmod = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "    lmod.fit(X_train, y_train)\n",
    "\n",
    "    fav_idx = np.where(lmod.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "    y_train_pred_prob = lmod.predict_proba(X_train)[:,fav_idx]\n",
    "\n",
    "    # Prediction probs for validation and testing data\n",
    "    X_valid = dataset_orig_valid.features[:,:-1]\n",
    "    y_valid_pred_prob = lmod.predict_proba(X_valid)[:,fav_idx]\n",
    "\n",
    "    X_test = dataset_orig_test.features[:,:-1]\n",
    "    y_test_pred_prob = lmod.predict_proba(X_test)[:,fav_idx]\n",
    "\n",
    "    class_thresh = 0.5\n",
    "    dataset_orig_train_pred.scores = y_train_pred_prob.reshape(-1,1)\n",
    "    dataset_orig_valid_pred.scores = y_valid_pred_prob.reshape(-1,1)\n",
    "    dataset_orig_test_pred.scores = y_test_pred_prob.reshape(-1,1)\n",
    "\n",
    "    y_train_pred = np.zeros_like(dataset_orig_train_pred.labels)\n",
    "    y_train_pred[y_train_pred_prob >= class_thresh] = dataset_orig_train_pred.favorable_label\n",
    "    y_train_pred[~(y_train_pred_prob >= class_thresh)] = dataset_orig_train_pred.unfavorable_label\n",
    "    dataset_orig_train_pred.labels = y_train_pred\n",
    "\n",
    "    y_valid_pred = np.zeros_like(dataset_orig_valid_pred.labels)\n",
    "    y_valid_pred[y_valid_pred_prob >= class_thresh] = dataset_orig_valid_pred.favorable_label\n",
    "    y_valid_pred[~(y_valid_pred_prob >= class_thresh)] = dataset_orig_valid_pred.unfavorable_label\n",
    "    dataset_orig_valid_pred.labels = y_valid_pred\n",
    "\n",
    "    y_test_pred = np.zeros_like(dataset_orig_test_pred.labels)\n",
    "    y_test_pred[y_test_pred_prob >= class_thresh] = dataset_orig_test_pred.favorable_label\n",
    "    y_test_pred[~(y_test_pred_prob >= class_thresh)] = dataset_orig_test_pred.unfavorable_label\n",
    "    dataset_orig_test_pred.labels = y_test_pred\n",
    "\n",
    "    \n",
    "    cpp = EqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     seed=randseed)\n",
    "    cpp = cpp.fit(dataset_orig_valid, dataset_orig_valid_pred)\n",
    "    \n",
    "    dataset_transf_valid_pred = cpp.predict(dataset_orig_valid_pred)\n",
    "    dataset_transf_test_pred = cpp.predict(dataset_orig_test_pred)\n",
    "    \n",
    "    cm_pred_test = ClassificationMetric(dataset_orig_test, dataset_orig_test_pred,\n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    display(Markdown(\"#### Original-Predicted testing dataset\"))\n",
    "    print(\"Difference in FPR between unprivileged and privileged groups\")\n",
    "    print(cm_pred_test.difference(cm_pred_test.false_negative_rate))\n",
    "    \n",
    "    print(\"Overall Test Accuracy \")\n",
    "    print(cm_pred_test.accuracy())\n",
    "    \n",
    "    cm_transf_test = ClassificationMetric(dataset_orig_test, dataset_transf_test_pred,\n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "    display(Markdown(\"#### Original-Transformed testing dataset\"))\n",
    "    print(\"Difference in FPR between unprivileged and privileged groups\")\n",
    "    fr = np.abs(cm_transf_test.difference(cm_transf_test.false_negative_rate))\n",
    "    fr_list.append(fr)\n",
    "    print(fr)\n",
    "\n",
    "    print(\"Overall Test Accuracy \")\n",
    "    acc = cm_transf_test.accuracy()\n",
    "    acc_list.append(acc)\n",
    "    print(acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean = [np.array(acc_list).mean()]\n",
    "acc_std = [np.array(acc_list).std()]\n",
    "fr_mean = [np.array(fr_list).mean()]\n",
    "fr_std = [np.array(fr_list).std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mean_eqodds_result.pkl', 'wb+') as f: \n",
    "    pickle.dump({'fr_mean': fr_mean, 'fr_std': fr_std, 'acc_mean': acc_mean, 'acc_std': acc_std}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
