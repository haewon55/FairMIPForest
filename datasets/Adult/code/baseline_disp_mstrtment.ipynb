{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Adult] Baseline -- Disparate Mistreatment DCCP (Zafar et al.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "sys.path.append('../../../fair-classification_python3/fair_classification') # the code for fair classification is in this directory\n",
    "sys.path.append(\"../\")\n",
    "cwd = '../../../core'\n",
    "sys.path.append(cwd)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils as ut\n",
    "import funcs_disp_mist as fdm\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from load_adult import * \n",
    "from missing_module import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Data ## \n",
    "df_train, df_test = load_adult()\n",
    "\n",
    "## Balancing the Data ##\n",
    "df = balance_data(df_train, 'income', 0)\n",
    "df = balance_data(df, 'gender', 1)\n",
    "\n",
    "sens_attr = 'gender'\n",
    "s = 42   # random seed\n",
    "\n",
    "## Generate Missing Data in Training Set ##\n",
    "df_ms = generate_missing(df, sens_attr, ms_label='marital-status', p_ms0=0, p_ms1=0.4, seed=s)\n",
    "df_ms = generate_missing(df_ms, sens_attr, ms_label='hours-per-week', p_ms0=0, p_ms1=0.3, seed=s)\n",
    "df_ms = generate_missing(df_ms, sens_attr, ms_label='race', p_ms0=0.2, p_ms1=0.2, seed=s)\n",
    "\n",
    "## Changing label 0 to -1 to use Zafar's method ##\n",
    "df_ms['income'].replace({0: -1}, inplace=True)\n",
    "\n",
    "df_ms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fair = 'fpr'\n",
    "fr_mean, acc_mean, fr_std, acc_std = [], [], [], [] \n",
    "tau_list = [0.01, 5, 50, 500, 5000]\n",
    "\n",
    "for tau in tau_list: \n",
    "    fr_list = []\n",
    "    acc_list = [] \n",
    "    \n",
    "    for seed in range (1, 11): \n",
    "       \n",
    "        ################## Train-Test Split ################### \n",
    "        dataset_orig_train, dataset_orig_test = train_test_split(df_ms, test_size=0.3, random_state=seed)\n",
    "\n",
    "        ########################################################\n",
    "\n",
    "        ##################### Imputation ###################### \n",
    "        \n",
    "        ## Change the following two lines to get mean or k-nn results ##\n",
    "#         imputer = SimpleImputer()\n",
    "        imputer = KNNImputer()\n",
    "        imputer.fit(dataset_orig_train)\n",
    "        df_train = pd.DataFrame(imputer.transform(dataset_orig_train), columns=dataset_orig_train.columns, \n",
    "                                                  index=dataset_orig_train.index)\n",
    "\n",
    "        df_test = pd.DataFrame(imputer.transform(dataset_orig_test), columns=dataset_orig_test.columns, \n",
    "                                                  index=dataset_orig_test.index)\n",
    "\n",
    "        ########################################################\n",
    "        sensitive_attrs = [sens_attr]\n",
    "\n",
    "        X_train = df_train.iloc[:,:-1]\n",
    "        X_train['intercept'] = np.ones(len(X_train))\n",
    "        x_control_train = dict({sens_attr: np.array([int(x) for x in X_train[sens_attr]])})\n",
    "        X_train = np.array(X_train.drop(columns=[sens_attr]))\n",
    "        y_train = np.array(df_train.iloc[:,-1])\n",
    "\n",
    "        X_test = df_test.iloc[:,:-1]\n",
    "        X_test['intercept'] = np.ones(len(X_test))\n",
    "        x_control_test = dict({sens_attr: np.array([int(x) for x in X_test[sens_attr]])})\n",
    "        X_test = np.array(X_test.drop(columns=[sens_attr]))\n",
    "        y_test = np.array(df_test.iloc[:,-1])\n",
    "\n",
    "\n",
    "        ##########################################################\n",
    "\n",
    "        ################ Disparate Mistreatment ################## \n",
    "        if fair == 'fpr':\n",
    "            cons_type = 1 \n",
    "        elif fair == 'fnr': \n",
    "            cons_type = 2\n",
    "        elif fair == 'acc':\n",
    "            cons_type = 0 \n",
    "        elif fair == 'eqodds':\n",
    "            cons_type = 4 \n",
    "\n",
    "        mu = 1.2\n",
    "        loss_function = \"logreg\" # perform the experiments with logistic regression\n",
    "        EPS = 1e-6\n",
    "\n",
    "        print(tau, seed)\n",
    "        sensitive_attrs_to_cov_thresh = {sens_attr: {0:{0:0, 1:0}, 1:{0:0, 1:0}, 2:{0:0, 1:0}} } # zero covariance threshold, means try to get the fairest solution\n",
    "        cons_params = {\"cons_type\": cons_type, \n",
    "                       \"tau\": tau, \n",
    "                       \"mu\": mu, \n",
    "                       \"sensitive_attrs_to_cov_thresh\": sensitive_attrs_to_cov_thresh}\n",
    "\n",
    "         \n",
    "        w = fdm.train_model_disp_mist(X_train, y_train, x_control_train, loss_function, EPS, cons_params)\n",
    "        train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test = fdm.get_clf_stats(w, X_train, y_train, x_control_train, X_test, y_test, x_control_test, sensitive_attrs)\n",
    "\n",
    "        fr = np.abs(s_attr_to_fp_fn_test[sens_attr][0][fair] - s_attr_to_fp_fn_test[sens_attr][1][fair])\n",
    "        fr_list.append(fr)\n",
    "        acc_list.append(test_score)\n",
    "        \n",
    "    fr_mean.append(np.mean(fr_list))\n",
    "    fr_std.append(np.std(fr_list))\n",
    "    acc_mean.append(np.mean(acc_list))\n",
    "    acc_std.append(np.std(acc_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('results/mean_disp_mistrtment_result.pkl', 'wb+') as f: \n",
    "with open('results/knn_disp_mistrtment_result.pkl', 'wb+') as f: \n",
    "    pickle.dump({'fr_mean': fr_mean, 'fr_std': fr_std, 'acc_mean': acc_mean, 'acc_std': acc_std}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
